{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#things associated with a label \n",
    "\n",
    "1- counter \n",
    "2- probability \n",
    "3- function for individual probability \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so when you create a class, you're not defining an object you're defining like something that that tells the operating length or the language that you're working in, what objects of this type look like. So for us objects of this type have a counter objects of this type have a probability and object of this type have a certain handful of functions associated with them. And, and that's what we'll go sort of capture in this in this in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelClass: \n",
    "    def __init__(self, label):\n",
    "        self.counter= collections.Counter()\n",
    "        self.label= label \n",
    "\n",
    "    def calc_probs(self, alpha):\n",
    "        self.probs = {key: (alpha + value) / (sum(self.counter.values()) + (alpha * len(self.counter))) for key, value in self.counter.items()}\n",
    "\n",
    "    def calc_prob_sen(self, sentence, label_prob):\n",
    "        return math.prod([self.probs.get(word,0) for word in sentence])*label_prob[self.label]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_train  = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/train.tsv'\n",
    "\n",
    "df_train = pd.read_table(uri_train,\n",
    "                             names = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c',\t'venue'])\n",
    "\n",
    "\n",
    "labels= ['true', \"false\", \"half-true\", \"barely-true\"]\n",
    "other_df = pd.DataFrame({\"label\": labels})\n",
    "\n",
    "df_linear = df_train[df_train.label.isin(other_df.label)]\n",
    "\n",
    "df_linear_1= df_linear.reset_index()\n",
    "df_linear_1['statement'] = df_linear_1['statement'].str.lower()\n",
    "#df_linear_1['statement']= df_linear_1['statement'].str.split()\n",
    "\n",
    "label_dict= {}\n",
    "\n",
    "for i in labels: \n",
    "\n",
    "    label_dict[i]= LabelClass(i)\n",
    "\n",
    "#print(label_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_test  = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/test.tsv'\n",
    "\n",
    "df_test = pd.read_table(uri_train,\n",
    "                             names = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c',\t'venue'])\n",
    "\n",
    "other_df_1 = pd.DataFrame({\"label\": ['true', \"false\", \"half-true\", \"barely-true\"]})\n",
    "\n",
    "df_linear_test = df_test[df_test.label.isin(other_df_1.label)]\n",
    "\n",
    "df_linear_test_1= df_linear_test.reset_index()\n",
    "df_linear_test_1['statement'] = df_linear_test_1['statement'].str.lower()\n",
    "#df_linear_1['statement']= df_linear_1['statement'].str.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_alpha(train_df, test_df, alpha: int): \n",
    "\n",
    "    df_linear_1 = train_df\n",
    "    df_linear_test_1 = test_df\n",
    "\n",
    "    ###################################################################\n",
    "    label_cnt = collections.Counter(df_linear_1.label)\n",
    "    # true_cnt = collections.Counter() #true_cnt is p(x|true)\n",
    "    # false_cnt = collections.Counter()\n",
    "    # half_true_cnt= collections.Counter()\n",
    "    # barely_true_cnt= collections.Counter()\n",
    "\n",
    "\n",
    "\n",
    "    for idx, row in df_linear_1.iterrows():\n",
    "\n",
    "        counter= label_dict[row[\"label\"]].counter\n",
    "        counter.update(row[\"statement\"].split())\n",
    "\n",
    "\n",
    "    ################################################################### \n",
    "    label_probs = {key: value / sum(label_cnt.values()) for key, value in label_cnt.items()}\n",
    "\n",
    "    for key, value in label_dict.items():\n",
    "        value.calc_probs(alpha)\n",
    "\n",
    "    ##############L####################################################\n",
    "\n",
    "    to_calc =[]\n",
    "\n",
    "    for idx, row in df_linear_test_1.iterrows(): \n",
    "\n",
    "        sentence = row[\"statement\"].split() \n",
    "        prob_arr = []\n",
    "        for label in labels:\n",
    "            label_obj = label_dict[label]\n",
    "            prob_arr.append(label_obj.calc_prob_sen(sentence, label_probs))\n",
    "        # probability_matrix = np.array([label_arr, prob_arr])\n",
    "        max_prob_index = np.argmax(prob_arr)\n",
    "        max_prob_label = labels[max_prob_index]\n",
    "        curr_probs = max_prob_label\n",
    "\n",
    "\n",
    "        #this is to calc accurracy later \n",
    "        if curr_probs == row[\"label\"]: \n",
    "            to_calc.append(1)\n",
    "        else: \n",
    "            to_calc.append(0)\n",
    "\n",
    "    ###########################################################################\n",
    "\n",
    "    count_correct = to_calc.count(1)\n",
    "    rows_count = df_linear_test_1.shape[0]\n",
    "    accuracy = count_correct/rows_count\n",
    "\n",
    "\n",
    "\n",
    "    return(accuracy) \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9154456244118833"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_alpha(df_linear_1, df_linear_test_1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "empt_list= []\n",
    "\n",
    "for i in range(100): \n",
    "\n",
    "    curr_accurange= naive_bayes_alpha(df_linear_1, df_linear_test_1, i)\n",
    "\n",
    "    curr_list= [i,curr_accurange]\n",
    "\n",
    "    empt_list.append(curr_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothing has not helped. the accuracy keeps on dropping \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

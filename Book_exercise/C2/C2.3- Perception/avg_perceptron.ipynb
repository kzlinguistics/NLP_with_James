{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_train  = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/train.tsv'\n",
    "\n",
    "df_train = pd.read_table(uri_train,\n",
    "                             names = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c',\t'venue'])\n",
    "\n",
    "other_df = pd.DataFrame({\"label\": ['true', \"false\"]})\n",
    "\n",
    "df_linear = df_train[df_train.label.isin(other_df.label)]\n",
    "\n",
    "df_linear_1= df_linear.reset_index()\n",
    "df_linear_1['statement'] = df_linear_1['statement'].str.lower()\n",
    "#df_linear_1['statement']= df_linear_1['statement'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_test  = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/test.tsv'\n",
    "\n",
    "df_test = pd.read_table(uri_train,\n",
    "                             names = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c',\t'venue'])\n",
    "\n",
    "other_df_1 = pd.DataFrame({\"label\": ['true', \"false\"]})\n",
    "\n",
    "df_linear_test = df_test[df_test.label.isin(other_df_1.label)]\n",
    "\n",
    "df_linear_test_1= df_linear_test.reset_index()\n",
    "df_linear_test_1['statement'] = df_linear_test_1['statement'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear_1['tokenized_statement'] = df_linear_1.apply(lambda row: nltk.word_tokenize(row['statement']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a vocab \n",
    "\n",
    "#all the unique words in a dataset \n",
    "word_set = set()\n",
    "\n",
    "for idx, row in df_linear_1.iterrows():\n",
    "    word_set.update(row['tokenized_statement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a look up table \n",
    "vocab = {word:idx for idx,word in enumerate(word_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_zeros_needed = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_generator(sequence_toekns, vocab): \n",
    "\n",
    "    return_vector= np.zeros(number_of_zeros_needed)\n",
    "\n",
    "    #set some positions within that vector \n",
    "    for token in  sequence_toekns: \n",
    "        pos = vocab.get(token)\n",
    "        if pos is not None:\n",
    "            return_vector[pos] +=1\n",
    "\n",
    "\n",
    "\n",
    "    return(return_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_perceptron(df_train, df_test):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    #weights for the model \n",
    "    theta_true= np.zeros(number_of_zeros_needed)\n",
    "    theta_false= np.zeros(number_of_zeros_needed)\n",
    "\n",
    "    avg_weight_true=np.zeros(number_of_zeros_needed) \n",
    "    avg_weight_false=np.zeros(number_of_zeros_needed) \n",
    "\n",
    "\n",
    "    while count<100:\n",
    "\n",
    "        df_train_sample= df_train.sample()\n",
    "\n",
    "        #get us the lable \n",
    "        actual_label = df_train_sample[\"label\"].iat[0]\n",
    "\n",
    "        #one hot vector for sentence \n",
    "        x_vector= vector_generator(df_train_sample[\"tokenized_statement\"].iat[0],vocab)\n",
    "\n",
    "        #To James- why the .dot operation?? \n",
    "\n",
    "        #for i in range(len(theta)):\n",
    "\n",
    "        #calc scores for algo \n",
    "        theta_true_in_loop = np.dot(x_vector,theta_true)\n",
    "        theta_false_in_loop= np.dot(x_vector,theta_false)\n",
    "\n",
    "        predicted_label = \"true\" if theta_true_in_loop >= theta_false_in_loop else \"false\"\n",
    "\n",
    "        if actual_label != predicted_label:\n",
    "\n",
    "            print(\"wrong\")\n",
    "\n",
    "            #To James- need more explanation on this part \n",
    "            if actual_label == \"false\":\n",
    "                \n",
    "                #34:08 \n",
    "                #penalize if mistake and encourage if right \n",
    "                #we want theta false to go higher \n",
    "\n",
    "                theta_false = theta_false +x_vector #I want the actual label to have more weight on those words \n",
    "                theta_true = theta_true -x_vector \n",
    "            else: \n",
    "                theta_false = theta_false -x_vector \n",
    "                theta_true = theta_true +x_vector \n",
    "        else:\n",
    "            print(\"right\")\n",
    "\n",
    "        avg_weight_true += theta_true\n",
    "        avg_weight_false+= theta_false\n",
    "\n",
    "        \n",
    "        count+=1 \n",
    "\n",
    "\n",
    "    return avg_weight_true/count, avg_weight_false/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n"
     ]
    }
   ],
   "source": [
    "avg_weight_true,avg_weight_false= avg_perceptron(df_linear_1, df_linear_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   5,   16,   22,   32,  103,  119,  122,  124,  132,  136,  143,\n",
      "        146,  177,  194,  204,  222,  237,  241,  257,  272,  274,  287,\n",
      "        297,  359,  367,  377,  409,  436,  465,  478,  488,  522,  543,\n",
      "        556,  557,  570,  571,  586,  593,  599,  600,  621,  624,  672,\n",
      "        689,  710,  713,  756,  763,  767,  769,  770,  774,  777,  797,\n",
      "        799,  802,  807,  812,  861,  868,  913,  914,  925,  937,  967,\n",
      "        975,  993, 1000, 1003, 1016, 1064, 1100, 1101, 1124, 1131, 1134,\n",
      "       1146, 1201, 1209, 1224, 1272, 1273, 1279, 1295, 1314, 1317, 1347,\n",
      "       1372, 1427, 1437, 1447, 1454, 1466, 1475, 1480, 1483, 1491, 1572,\n",
      "       1576, 1624, 1636, 1644, 1645, 1647, 1668, 1688, 1750, 1755, 1793,\n",
      "       1795, 1797, 1800, 1811, 1822, 1862, 1865, 1886, 1888, 1916, 1940,\n",
      "       1941, 1952, 1974, 2018, 2020, 2049, 2052, 2063, 2113, 2121, 2186,\n",
      "       2208, 2211, 2212, 2255, 2263, 2275, 2278, 2302, 2318, 2339, 2386,\n",
      "       2409, 2426, 2435, 2453, 2463, 2495, 2513, 2531, 2532, 2544, 2557,\n",
      "       2560, 2564, 2567, 2568, 2573, 2608, 2637, 2680, 2685, 2698, 2747,\n",
      "       2757, 2759, 2791, 2793, 2809, 2814, 2849, 2852, 2883, 2975, 2985,\n",
      "       2992, 3001, 3019, 3040, 3051, 3065, 3074, 3120, 3136, 3141, 3168,\n",
      "       3177, 3190, 3202, 3216, 3281, 3294, 3324, 3364, 3382, 3410, 3424,\n",
      "       3460, 3466, 3469, 3477, 3497, 3519, 3550, 3555, 3567, 3572, 3621,\n",
      "       3623, 3655, 3656, 3668, 3695, 3701, 3710, 3733, 3747, 3766, 3769,\n",
      "       3770, 3794, 3820, 3855, 3858, 3875, 3888, 3891, 3947, 3951, 3962,\n",
      "       3975, 3980, 4004, 4041, 4043, 4055, 4082, 4083, 4106, 4117, 4122,\n",
      "       4130, 4133, 4171, 4176, 4185, 4194, 4209, 4222, 4224, 4287, 4289,\n",
      "       4291, 4305, 4307, 4333, 4336, 4338, 4357, 4385, 4397, 4404, 4411,\n",
      "       4432, 4461, 4468, 4504, 4513, 4517, 4557, 4559, 4560, 4583, 4619,\n",
      "       4623, 4624, 4634, 4644, 4674, 4735, 4768, 4774, 4789, 4798, 4829,\n",
      "       4852, 4858, 4920, 4928, 4937, 4954, 4955, 4969, 4980, 5014, 5021,\n",
      "       5052, 5060, 5064, 5078, 5090, 5091, 5100, 5105, 5128, 5129, 5130,\n",
      "       5148, 5169, 5193, 5194, 5195, 5201, 5212, 5218, 5222, 5260, 5289,\n",
      "       5295, 5300, 5302, 5315, 5351, 5355, 5358, 5369, 5380, 5412, 5421,\n",
      "       5430, 5443, 5558, 5561, 5606, 5615, 5627, 5631, 5637, 5638, 5665,\n",
      "       5683, 5689, 5712, 5723, 5768, 5831, 5834, 5854, 5865, 5870, 5879,\n",
      "       5916, 5927, 5935, 5958, 5984, 5989, 6009, 6011, 6047, 6080, 6127,\n",
      "       6147, 6164, 6169, 6188, 6199, 6202, 6204, 6212, 6219, 6276, 6293,\n",
      "       6298, 6302, 6323, 6330, 6336, 6342, 6352, 6356, 6360, 6367, 6387,\n",
      "       6389, 6410, 6414, 6469, 6488, 6511, 6520, 6531, 6580, 6619, 6725,\n",
      "       6729, 6750, 6761, 6763, 6779, 6792, 6805, 6809, 6832, 6867, 6916,\n",
      "       6950, 6963, 6965, 6982, 6986, 6992, 7005, 7017, 7035, 7073, 7078,\n",
      "       7080, 7091, 7149, 7171, 7182, 7210, 7251, 7268, 7282, 7288, 7327,\n",
      "       7355, 7356, 7380, 7396, 7401, 7407, 7420, 7423, 7432, 7439, 7446,\n",
      "       7521, 7540, 7548, 7654, 7662, 7694, 7724, 7761, 7763, 7817, 7829,\n",
      "       7858, 7908, 7937, 7939, 7940, 7966, 7973, 7975, 7983, 7987, 8012]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.nonzero(avg_weight_true)) #these are indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from calendar import c\n",
    "\n",
    "\n",
    "for i,row in df_test.iterrows():\n",
    "\n",
    "    #get us the lable \n",
    "    actual_label = row[\"label\"].iat[0]\n",
    "\n",
    "    #one hot vector for sentence \n",
    "    x_vector= vector_generator(row[\"tokenized_statement\"].iat[0],vocab)\n",
    "\n",
    "    #To James- why the .dot operation?? \n",
    "\n",
    "    #for i in range(len(theta)):\n",
    "\n",
    "    #calc scores for algo \n",
    "    theta_true_in_loop = np.dot(x_vector, avg_weight_true ) #averaged weight\n",
    "    theta_false_in_loop= np.dot(x_vector, avg_weight_false)\n",
    "\n",
    "    predicted_label = \"true\" if theta_true_in_loop >= theta_false_in_loop else \"false\"\n",
    "\n",
    "    if actual_label != predicted_label:\n",
    "\n",
    "        print(\"wrong\")\n",
    "    else:\n",
    "        print(\"right\")\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58ed8a150d7d37ab663d3aba4c202cca1b056a70e46f6886df0072485b153cf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

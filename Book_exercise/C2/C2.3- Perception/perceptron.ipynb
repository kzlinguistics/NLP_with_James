{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_train  = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/train.tsv'\n",
    "\n",
    "df_train = pd.read_table(uri_train,\n",
    "                             names = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c',\t'venue'])\n",
    "\n",
    "other_df = pd.DataFrame({\"label\": ['true', \"false\"]})\n",
    "\n",
    "df_linear = df_train[df_train.label.isin(other_df.label)]\n",
    "\n",
    "df_linear_1= df_linear.reset_index()\n",
    "df_linear_1['statement'] = df_linear_1['statement'].str.lower()\n",
    "#df_linear_1['statement']= df_linear_1['statement'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_test  = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/test.tsv'\n",
    "\n",
    "df_test = pd.read_table(uri_train,\n",
    "                             names = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c',\t'venue'])\n",
    "\n",
    "other_df_1 = pd.DataFrame({\"label\": ['true', \"false\"]})\n",
    "\n",
    "df_linear_test = df_test[df_test.label.isin(other_df_1.label)]\n",
    "\n",
    "df_linear_test_1= df_linear_test.reset_index()\n",
    "df_linear_test_1['statement'] = df_linear_test_1['statement'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the perceptron algorithm, what we do is we continuously sample again and again, you pick an item at random from that data frame. See if it's right or wrong, make an update, pick another item. Then pick another one pick another one pick another one, and so on, keep making changes until the model converges. So there's no specific order we should be sampling or picking them. So we could we could pick all of them one time by calling the SROs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- tokenize all the input \n",
    "2- we need a feature vector for every item \n",
    "3- need to intiatialize our theta to be 0\n",
    "    3.1- the size of the theta needs to be the same size of the vocab \n",
    "    -|\\theta| = |V|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear_1['tokenized_statement'] = df_linear_1.apply(lambda row: nltk.word_tokenize(row['statement']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a vocab \n",
    "\n",
    "#all the unique words in a dataset \n",
    "word_set = set()\n",
    "\n",
    "for idx, row in df_linear_1.iterrows():\n",
    "    word_set.update(row['tokenized_statement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a look up table \n",
    "vocab = {word:idx for idx,word in enumerate(word_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_generator(sequence_toekns, vocab): \n",
    "\n",
    "    return_vector= np.zeros(number_of_zeros_needed)\n",
    "\n",
    "    #set some positions within that vector \n",
    "    for token in  sequence_toekns: \n",
    "        pos = vocab.get(token)\n",
    "        if pos is not None:\n",
    "            return_vector[pos] +=1\n",
    "\n",
    "\n",
    "\n",
    "    return(return_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(df_train, df_test):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    number_of_zeros_needed = len(vocab)\n",
    "\n",
    "    theta_true= np.zeros(number_of_zeros_needed)\n",
    "\n",
    "    theta_false= np.zeros(number_of_zeros_needed)\n",
    "     \n",
    "    while count<20:\n",
    "\n",
    "        df_train_sample= df_train.sample()\n",
    "\n",
    "        actual_label = df_train_sample[\"label\"].iat[0]\n",
    "        x_vector= vector_generator(df_train_sample[\"tokenized_statement\"].iat[0],vocab)\n",
    "\n",
    "        theta_true_in_loop = np.dot(x_vector,theta_true)\n",
    "        theta_false_in_loop= np.dot(x_vector,theta_false)\n",
    "\n",
    "        predicted_label = \"true\" if theta_true_in_loop >= theta_false_in_loop else \"false\"\n",
    "\n",
    "        if actual_label != predicted_label:\n",
    "\n",
    "            print(\"wrong\")\n",
    "\n",
    "            if actual_label == \"false\":\n",
    "\n",
    "                theta_false = theta_false +x_vector \n",
    "                theta_true = theta_true -x_vector \n",
    "\n",
    "            else: \n",
    "                theta_false = theta_false -x_vector \n",
    "                theta_true = theta_true +x_vector \n",
    "        else:\n",
    "            print(\"right\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        count+=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'theta_true' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kevin/Documents/NLP_with_James/Book_exercise/C2/C2.3- Perception/perceptron.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kevin/Documents/NLP_with_James/Book_exercise/C2/C2.3-%20Perception/perceptron.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m perceptron(df_linear_1, df_linear_test_1)\n",
      "\u001b[1;32m/home/kevin/Documents/NLP_with_James/Book_exercise/C2/C2.3- Perception/perceptron.ipynb Cell 12\u001b[0m in \u001b[0;36mperceptron\u001b[0;34m(df_train, df_test)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kevin/Documents/NLP_with_James/Book_exercise/C2/C2.3-%20Perception/perceptron.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m actual_label \u001b[39m=\u001b[39m df_train_sample[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miat[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevin/Documents/NLP_with_James/Book_exercise/C2/C2.3-%20Perception/perceptron.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m x_vector\u001b[39m=\u001b[39m vector_generator(df_train_sample[\u001b[39m\"\u001b[39m\u001b[39mtokenized_statement\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miat[\u001b[39m0\u001b[39m],vocab)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kevin/Documents/NLP_with_James/Book_exercise/C2/C2.3-%20Perception/perceptron.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m theta_true_in_loop \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(x_vector,theta_true)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevin/Documents/NLP_with_James/Book_exercise/C2/C2.3-%20Perception/perceptron.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m theta_false_in_loop\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(x_vector,theta_false)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kevin/Documents/NLP_with_James/Book_exercise/C2/C2.3-%20Perception/perceptron.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m predicted_label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m theta_true_in_loop \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m theta_false_in_loop \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'theta_true' referenced before assignment"
     ]
    }
   ],
   "source": [
    "perceptron(df_linear_1, df_linear_test_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58ed8a150d7d37ab663d3aba4c202cca1b056a70e46f6886df0072485b153cf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

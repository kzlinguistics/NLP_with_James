{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_train  = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/train.tsv'\n",
    "\n",
    "df_train = pd.read_table(uri_train,\n",
    "                             names = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c',\t'venue'])\n",
    "\n",
    "other_df = pd.DataFrame({\"label\": ['true', \"false\"]})\n",
    "\n",
    "df_linear = df_train[df_train.label.isin(other_df.label)]\n",
    "\n",
    "df_linear_1= df_linear.reset_index()\n",
    "df_linear_1['statement'] = df_linear_1['statement'].str.lower()\n",
    "#df_linear_1['statement']= df_linear_1['statement'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_test  = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/test.tsv'\n",
    "\n",
    "df_test = pd.read_table(uri_train,\n",
    "                             names = ['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\t'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\t'pants_on_fire_c',\t'venue'])\n",
    "\n",
    "other_df_1 = pd.DataFrame({\"label\": ['true', \"false\"]})\n",
    "\n",
    "df_linear_test = df_test[df_test.label.isin(other_df_1.label)]\n",
    "\n",
    "df_linear_test_1= df_linear_test.reset_index()\n",
    "df_linear_test_1['statement'] = df_linear_test_1['statement'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the perceptron algorithm, what we do is we continuously sample again and again, you pick an item at random from that data frame. See if it's right or wrong, make an update, pick another item. Then pick another one pick another one pick another one, and so on, keep making changes until the model converges. So there's no specific order we should be sampling or picking them. So we could we could pick all of them one time by calling the SROs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- tokenize all the input \n",
    "2- we need a feature vector for every item \n",
    "3- need to intiatialize our theta to be 0\n",
    "    3.1- the size of the theta needs to be the same size of the vocab \n",
    "    -|\\theta| = |V|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linear_1['tokenized_statement'] = df_linear_1.apply(lambda row: nltk.word_tokenize(row['statement']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a vocab \n",
    "\n",
    "#all the unique words in a dataset \n",
    "word_set = set()\n",
    "\n",
    "for idx, row in df_linear_1.iterrows():\n",
    "    word_set.update(row['tokenized_statement'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you consider the, the weight, so that I got the number of the word for a movie, for example, and get the position in that data vector for the word movie, And now correspond to the strength or the informativeness of that word for your task as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a look up table \n",
    "\n",
    "#how many times did a word occur \n",
    "\n",
    "#what position of the vector correspond to the word \n",
    "#consider the id number \n",
    "\n",
    "vocab = {word:idx for idx,word in enumerate(word_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_zeros_needed = len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we using a vector generator? \n",
    "\n",
    "23:00\n",
    "is a vector, right? so you've got some theta, which is a vector of weights and then you've got your your f of x which is a feature generator, which needs to output a vector the same size as the weights which you're then taking the inner product of, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_generator(sequence_toekns, vocab): \n",
    "\n",
    "    return_vector= np.zeros(number_of_zeros_needed)\n",
    "\n",
    "    #set some positions within that vector \n",
    "    for token in  sequence_toekns: \n",
    "        pos = vocab.get(token)\n",
    "        if pos is not None:\n",
    "            return_vector[pos] +=1\n",
    "\n",
    "\n",
    "\n",
    "    return(return_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(df_train, df_test):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    #weights for the model \n",
    "    theta_true= np.zeros(number_of_zeros_needed)\n",
    "    theta_false= np.zeros(number_of_zeros_needed)\n",
    "     \n",
    "    while count<100:\n",
    "\n",
    "        df_train_sample= df_train.sample()\n",
    "\n",
    "        #get us the lable \n",
    "        actual_label = df_train_sample[\"label\"].iat[0]\n",
    "\n",
    "        #one hot vector for sentence \n",
    "        x_vector= vector_generator(df_train_sample[\"tokenized_statement\"].iat[0],vocab)\n",
    "\n",
    "        #To James- why the .dot operation?? \n",
    "\n",
    "        #for i in range(len(theta)):\n",
    "\n",
    "        #calc scores for algo \n",
    "        theta_true_in_loop = np.dot(x_vector,theta_true)\n",
    "        theta_false_in_loop= np.dot(x_vector,theta_false)\n",
    "\n",
    "        predicted_label = \"true\" if theta_true_in_loop >= theta_false_in_loop else \"false\"\n",
    "\n",
    "        if actual_label != predicted_label:\n",
    "\n",
    "            print(\"wrong\")\n",
    "\n",
    "            #To James- need more explanation on this part \n",
    "            if actual_label == \"false\":\n",
    "                \n",
    "                #34:08 \n",
    "                #penalize if mistake and encourage if right \n",
    "                #we want theta false to go higher \n",
    "\n",
    "                theta_false = theta_false +x_vector #I want the actual label to have more weight on those words \n",
    "                theta_true = theta_true -x_vector \n",
    "\n",
    "            else: \n",
    "                theta_false = theta_false -x_vector \n",
    "                theta_true = theta_true +x_vector \n",
    "        else:\n",
    "            print(\"right\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        count+=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "wrong\n",
      "right\n",
      "wrong\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "perceptron(df_linear_1, df_linear_test_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58ed8a150d7d37ab663d3aba4c202cca1b056a70e46f6886df0072485b153cf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
